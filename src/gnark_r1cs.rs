//! SP1 fork dumps r1cs matrix and witness generated by gnark
//! We read from that file appropriately through functions below
//! ```text
//!                        SP‑1 dump binary layout                       
//!  little‑endian
//!  ──────────────────────────────────────────────────────────────────────
//!  u32  nbCoeffs                        ┐  (#entries in coefficient table)
//!  32B coeff[0]                         │
//!  32B coeff[1]                         │  (32‑byte BE values)
//!  ...                                  ├─ coeff_table (length = nbCoeffs)
//!  32B coeff[nbCoeffs‑1]                │
//!
//!  u32  nbRows                          ┐
//!  repeat nbRows times:                 │
//!      u32 nL | u32 nR | u32 nO         │ counts of terms for this row
//!      nL × Term | nR × Term | nO × Term│  (see `Term`)
//!
//!  Term =  (u32 wire_id, u32 coeff_id)  │ little‑endian
//!  ──────────────────────────────────────────────────────────────────────
//! ```

use blake3::Hasher;
use num_bigint::BigUint;
use num_traits::cast::FromPrimitive;
use rayon::{
    iter::{IntoParallelIterator, ParallelIterator},
    scope,
};
use std::{
    fs::File,
    io::{self, BufReader, Read},
};

const BYTES: usize = 32;

// Field element type (4 × u64 limbs).
#[derive(Clone, Copy, Default, Debug, Eq, PartialEq)]
pub(crate) struct Element(pub [u64; 4]);
struct BigEndian;

impl BigEndian {
    /// Decode an element from a 32-byte big-endian buffer.
    fn element(buf: &[u8; BYTES]) -> io::Result<Element> {
        let mut z = Element::default();
        z.0[0] = u64::from_be_bytes(buf[24..32].try_into().unwrap());
        z.0[1] = u64::from_be_bytes(buf[16..24].try_into().unwrap());
        z.0[2] = u64::from_be_bytes(buf[8..16].try_into().unwrap());
        z.0[3] = u64::from_be_bytes(buf[0..8].try_into().unwrap());
        Ok(z)
    }
}

pub(crate) type Vector = Vec<Element>;

pub(crate) trait ReadFrom {
    fn read_from<R: Read>(&mut self, r: &mut R) -> io::Result<u64>;
}

impl ReadFrom for Vector {
    fn read_from<R: Read>(&mut self, r: &mut R) -> io::Result<u64> {
        let mut len_bytes = [0u8; 4];
        r.read_exact(&mut len_bytes)?;
        let slice_len = u32::from_be_bytes(len_bytes) as usize;

        self.clear();
        self.reserve_exact(slice_len);

        let mut read: u64 = 4;
        let mut buf = [0u8; BYTES];
        for _ in 0..slice_len {
            r.read_exact(&mut buf)?;
            let e = BigEndian::element(&buf)?;
            self.push(e);
            read += BYTES as u64;
        }
        Ok(read)
    }
}

use ark_ff::{AdditiveGroup, Field, PrimeField};
use byteorder::{LittleEndian, ReadBytesExt};

use crate::curve::Fr;

// Binary layout (little-endian):
//   [u32] nbCoeffs
//   nbCoeffs × [32]u8
//   [u32] nbRows
//   per row:
//       [u32] nL | [u32] nR | [u32] nO
//       nL × Term | nR × Term | nO × Term
//   Term = (wire_id u32, coeff_id u32)

/// One matrix entry in the naïve R1CS.
#[derive(Debug, Clone, Copy, PartialEq)]
pub struct Term {
    /// wire_id
    pub wire_id: u32,
    /// coeff_id
    pub coeff_id: u32,
}

/// A single constraint row (L, R, O).
#[derive(Debug, Clone, PartialEq)]
pub struct Row {
    /// l
    pub l: Vec<Term>,
    /// r
    pub r: Vec<Term>,
    /// o
    pub o: Vec<Term>,
}

/// Complete dump = coefficient table + rows.
#[derive(Debug, Clone, PartialEq)]
pub(crate) struct SparseR1CSTable {
    pub coeff_table: Vec<[u8; 32]>,
    pub indices_rows: Vec<Row>,
}

/// Parse a gnark SP‑1 sparse‑R1CS dump from a file
pub(crate) fn load_sparse_r1cs_from_file<R: Read + Send>(
    mut reader: R,
) -> io::Result<SparseR1CSTable> {
    // ───────── 1. coefficients (exactly n_coeffs × 32 bytes) ─────────────
    let n_coeffs = reader.read_u32::<LittleEndian>()?;
    let mut coeff_table = Vec::with_capacity(n_coeffs as usize);
    for _ in 0..n_coeffs {
        let mut buf = [0u8; 32];
        reader.read_exact(&mut buf)?;
        coeff_table.push(buf);
    }

    // ───────── 2. rows: read metadata & bytes serially, parse in parallel ─
    let n_rows = reader.read_u32::<LittleEndian>()? as usize;
    let mut rows = vec![
        Row {
            l: vec![],
            r: vec![],
            o: vec![]
        };
        n_rows
    ]; // placeholder

    scope(|s| {
        for slot in &mut rows {
            // Read counts for this row
            let n_l = reader.read_u32::<LittleEndian>().unwrap() as usize;
            let n_r = reader.read_u32::<LittleEndian>().unwrap() as usize;
            let n_o = reader.read_u32::<LittleEndian>().unwrap() as usize;
            let term_count = n_l + n_r + n_o;

            // Read exactly term_count × 8 bytes
            let mut buf = vec![0u8; term_count * 8];
            reader.read_exact(&mut buf).unwrap();

            // Kick a worker that parses this row’s buffer
            s.spawn(move |_| {
                // Helper to parse n terms from &mut slice, advancing cursor
                fn take_terms(chunk: &mut &[u8], n: usize) -> Vec<Term> {
                    (0..n)
                        .map(|_| {
                            let wire_id = u32::from_le_bytes(chunk[..4].try_into().unwrap());
                            let coeff_id = u32::from_le_bytes(chunk[4..8].try_into().unwrap());
                            *chunk = &chunk[8..];
                            Term { wire_id, coeff_id }
                        })
                        .collect()
                }

                let mut slice: &[u8] = &buf;
                let l = take_terms(&mut slice, n_l);
                let r = take_terms(&mut slice, n_r);
                let o = take_terms(&mut slice, n_o);

                // SAFETY: this slot is unique to the current iteration
                *slot = Row { l, r, o };
            });
        }
    });

    Ok(SparseR1CSTable {
        coeff_table,
        indices_rows: rows,
    })
}

/// load_witness_from_file
pub fn load_witness_from_file(witness_file_path: &str) -> Vec<Fr> {
    let f = File::open(witness_file_path).unwrap();
    let mut src = BufReader::new(f);
    let mut warr = Vector::default();
    warr.read_from(&mut src).unwrap();
    let wit_fr: Vec<Fr> = warr
        .into_par_iter()
        .map(|w| gnark_element_to_fr(&w))
        .collect();
    wit_fr
}

/// Convert a single limb-representation `Element` into an `Fr`.
pub(crate) fn gnark_element_to_fr(e: &Element) -> Fr {
    // Element limbs are stored little-endian:  [u64;4] == [lo, .. , hi].
    // Ark expects *big-endian* byte order for `from_be_bytes_mod_order`.
    let mut bytes = [0u8; 32];
    bytes[0..8].copy_from_slice(&e.0[3].to_be_bytes()); // most-significant limb
    bytes[8..16].copy_from_slice(&e.0[2].to_be_bytes());
    bytes[16..24].copy_from_slice(&e.0[1].to_be_bytes());
    bytes[24..32].copy_from_slice(&e.0[0].to_be_bytes()); // least-significant

    Fr::from_be_bytes_mod_order(&bytes)
}

/// generate sp1 public input scalar field element for given raw public input
pub fn sp1_generate_scalar_from_raw_public_input(raw_pub_input: u64) -> Fr {
    fn babybear_bytes_to_bn254(bytes: &[u8; 32]) -> BigUint {
        let mut result = BigUint::ZERO;
        for (idx, byte) in bytes.iter().enumerate() {
            result *= BigUint::from_u16(256).unwrap(); // shift by 7 bits
            let masked = if idx < 4 { 0 } else { *byte };
            result += BigUint::from_u8(masked).unwrap(); // add 7-bit
        }
        result
    }
    let mut hasher = Hasher::new();
    hasher.update(&raw_pub_input.to_le_bytes());
    let res = hasher.finalize();
    let pubinp_fr = babybear_bytes_to_bn254(res.as_bytes()); // hashed and truncated to 224 bits
    Fr::from(pubinp_fr)
}

#[cfg(test)]
pub(crate) mod sparse_verify_r1cs {
    use ark_ff::PrimeField;

    use crate::gnark_r1cs::Row;

    use super::*;
    use ark_std::Zero;

    // Evaluate ⟨expr, witness⟩
    pub(crate) fn eval(expr: &[Term], coeffs: &[[u8; 32]], w: &[Fr]) -> Fr {
        expr.iter().fold(Fr::zero(), |acc, t| {
            let coeff = coeff_as_fr(coeffs, t.coeff_id);
            acc + coeff * w[t.wire_id as usize]
        })
    }

    fn coeff_as_fr(coeffs: &[[u8; 32]], cid: u32) -> Fr {
        Fr::from_be_bytes_mod_order(&coeffs[cid as usize])
    }

    // Check a single row
    pub(crate) fn check_row(row: &Row, coeffs: &[[u8; 32]], w: &[Fr]) -> bool {
        let l = eval(&row.l, coeffs, w);
        let r = eval(&row.r, coeffs, w);
        let o = eval(&row.o, coeffs, w);
        l * r == o
    }
}

#[derive(Clone)]
pub(crate) struct R1CSInstance {
    pub num_constraints: usize,   // #constraints raised to the power of 2
    pub num_public_inputs: usize, // #public inputs (excl. leading 1)
    pub rows: Vec<Row>,           // len = m ; each Row has {l,r,o}: Vec<Term>
    pub coeffs: Vec<Fr>,          // len = #unique scalars
}

impl R1CSInstance {
    /// Evaluate a single R1CS Constraint (given by `terms` and `coeffs`) at witness `witness`
    // `coeffs` is coefficient table
    // `terms` represents a single constraint for any of L, R, O
    pub(crate) fn eval_row(terms: &[Term], coeffs: &[Fr], witness: &[Fr]) -> Fr {
        let mut acc = Fr::ZERO;
        for t in terms {
            let coeff = coeffs[t.coeff_id as usize];
            acc += coeff * witness[t.wire_id as usize];
        }
        acc
    }

    pub(crate) fn from_dump(r1cs_table: SparseR1CSTable, num_public_inputs: usize) -> Self {
        // decode coefficient bytes (canonical big-endian) ➜ Montgomery Fr
        let coeffs: Vec<Fr> = r1cs_table
            .coeff_table
            .iter()
            .map(|bytes| Fr::from_be_bytes_mod_order(bytes))
            .collect();

        Self {
            num_constraints: r1cs_table.indices_rows.len().next_power_of_two(),
            num_public_inputs,
            rows: r1cs_table.indices_rows,
            coeffs,
        }
    }

    pub(crate) fn initialize_with_vandermode_matrix_for_public_input_poly(
        path: &str,
        dom_elems: &[Fr],
        num_public_inputs: usize,
    ) -> R1CSInstance {
        let f = File::open(path).unwrap();
        let dump = load_sparse_r1cs_from_file(f).unwrap();
        let mut inst = R1CSInstance::from_dump(dump, num_public_inputs);
        R1CSInstance::update_to_include_vandermode_matrix_d(
            &mut inst,
            dom_elems,
            num_public_inputs,
        );
        inst
    }

    // R1CSInstance passed as input to this function adheres to the standard R1CS constraint Aw . Bw = Cw
    // Here 'w' is a witness vector of size 1 + num_public_inputs + num_private inputs and has the format
    // [1, ..public_inputs, private_inputs]. A, B and C are R1CS Matrices of size num_constraints x witness_size
    // In our case (SP1 Integration), both num_constraints and witness_size is slightly less than 1 << 23.

    // The purpose this function is to introduce matrix D such that Aw . Bw = C'w + Dx,
    // where x is public-input vector and Dx is evaluation of public input polynomial i(X) over the domain
    // and matrix D is of size num_constraints x num_public_inputs

    // To adjust to the new relation, we modify C to C' such that C'w = Cw - Dx
    // We do this by iterating over each of the constraints (rows) of C and add corresponding constraint of -D
    // This means changing the elements of matrix from C_{ij} to C_{ij} - D_{ij} for i < num_constraints and j < num_public_inputs

    // D is of size [num_constraints x num_public_inputs]
    // d0^0, d0^1, ..., d0^{k-1}
    // d1^0,
    // d2^0,
    // ...
    // d_{m-1}^0, ..., d_{m-1}^{k-1}
    pub(crate) fn update_to_include_vandermode_matrix_d(
        inst: &mut R1CSInstance,
        dom_elems: &[Fr],
        num_public_inputs: usize,
    ) {
        // find index of Fr::ONE, push and return the latest index if it doesn't exist
        let minus_one = -Fr::ONE;
        let coeff_one = inst.coeffs.iter().enumerate().find(|x| *x.1 == minus_one);
        let coeff_one_idx = if let Some(coeff_one) = coeff_one {
            coeff_one.0
        } else {
            inst.coeffs.push(minus_one);
            inst.coeffs.len() - 1 // latest index is of -Fr::ONE
        };

        // public inputs start from index 1 in witness vector, zero-th index is value 1
        let pub_inputs_wire_id: Vec<u32> = (0..num_public_inputs).map(|x| 1 + x as u32).collect();

        assert_eq!(dom_elems.len(), inst.num_constraints);
        for (i, dom_i) in dom_elems.iter().enumerate().take(inst.num_constraints) {
            if i >= inst.rows.len() {
                inst.rows.push(Row {
                    l: vec![],
                    r: vec![],
                    o: vec![],
                });
            }

            let mut increasing_power_of_d = *dom_i; // start with d^1
            //for j in 0..num_public_inputs {
            for (j, pub_wire_id) in pub_inputs_wire_id
                .iter()
                .enumerate()
                .take(num_public_inputs)
            {
                if j == 0 {
                    inst.rows[i].o.push(Term {
                        wire_id: *pub_wire_id,
                        coeff_id: coeff_one_idx as u32,
                    });
                } else {
                    inst.coeffs.push(-increasing_power_of_d); // -d^j
                    let new_coeff_idx = inst.coeffs.len() - 1; // last index
                    inst.rows[i].o.push(Term {
                        wire_id: *pub_wire_id,
                        coeff_id: new_coeff_idx as u32,
                    });

                    increasing_power_of_d *= *dom_i;
                    // -d^{j+1}
                }
            }
        }
    }
}

// evaluate monomial basis polynomial at alpha
// this polynomial has 'public_inputs' as its coefficients
pub(crate) fn evaluate_monomial_basis_poly(public_inputs: &[Fr], alpha: Fr) -> Fr {
    let mut increasing_power_of_alpha = Fr::ONE;
    let mut running_accumulator = Fr::ZERO;
    for pubin in public_inputs {
        running_accumulator += pubin * &increasing_power_of_alpha;
        increasing_power_of_alpha *= alpha; // [1, alpha, alpha^2, .., alpha^{num_public_inputs-1}]
    }
    running_accumulator
}

#[cfg(test)]
use byteorder::WriteBytesExt;

#[cfg(test)]
pub(crate) fn write_sparse_r1cs_to_file<W: std::io::Write>(
    mut writer: W,
    table: &SparseR1CSTable,
) -> std::io::Result<()> {
    // ───────── 1. coefficient table ────────────────────────────────────
    writer.write_u32::<LittleEndian>(table.coeff_table.len() as u32)?;
    for coeff in &table.coeff_table {
        writer.write_all(coeff)?; // already 32-byte big-endian chunks
    }

    // ───────── 2. rows ─────────────────────────────────────────────────
    writer.write_u32::<LittleEndian>(table.indices_rows.len() as u32)?;
    for row in &table.indices_rows {
        // counts
        writer.write_u32::<LittleEndian>(row.l.len() as u32)?;
        writer.write_u32::<LittleEndian>(row.r.len() as u32)?;
        writer.write_u32::<LittleEndian>(row.o.len() as u32)?;

        // Terms (L || R || O), each = wire_id || coeff_id (little-endian u32)
        let mut write_terms = |terms: &[Term]| -> std::io::Result<()> {
            for t in terms {
                writer.write_u32::<LittleEndian>(t.wire_id)?;
                writer.write_u32::<LittleEndian>(t.coeff_id)?;
            }
            Ok(())
        };
        write_terms(&row.l)?;
        write_terms(&row.r)?;
        write_terms(&row.o)?;
    }

    writer.flush()?;
    Ok(())
}

#[cfg(test)]
mod test {

    use std::path::Path;
    use std::{fs::File, io::BufReader};

    use crate::artifacts::{R1CS_CONSTRAINTS_FILE, R1CS_WITNESS_FILE, TREE_2N};
    use crate::ec_fft::get_both_domains;

    use crate::gnark_r1cs::{
        ReadFrom, Vector, gnark_element_to_fr, load_sparse_r1cs_from_file,
        sparse_verify_r1cs::check_row,
    };
    use crate::gnark_r1cs::{load_witness_from_file, sp1_generate_scalar_from_raw_public_input};

    use crate::curve::Fr;
    use crate::tree_io::read_minimal_fftree_from_file;

    use ark_ff::AdditiveGroup;
    use ecfft::FFTree;
    use rayon::iter::{IntoParallelIterator, ParallelIterator};

    use super::R1CSInstance;

    #[test]
    #[ignore]
    fn test_sample_check_r1cs_constraint() {
        let f = File::open("srs_sect/r1cs_temp").unwrap();
        let dump = load_sparse_r1cs_from_file(f).unwrap();

        println!("size_coeff_table {:?}", dump.coeff_table.len());

        let wit_fr: Vec<Fr> = load_witness_from_file("srs_sect/witness_temp");
        println!("wit_fr len {}", wit_fr.len());
        println!("row len {}", dump.indices_rows.len());

        let failures = 0usize;
        (0..dump.indices_rows.len())
            .into_par_iter()
            .for_each(|idx| {
                //dump.indices_rows.len() {
                if !check_row(&dump.indices_rows[idx], &dump.coeff_table, &wit_fr) {
                    eprintln!("row {} FAILS", idx);
                    panic!();
                }
            });

        if failures == 0 {
            println!("All sampled rows satisfied the R1CS relation.");
        } else {
            println!("{}  sampled rows FAILED the R1CS relation.", failures);
        }
    }

    #[test]
    #[ignore]
    fn test_public_inputs_hash() {
        let fibo_input = [55, 0, 0, 0, 89, 0, 0, 0]; // serialized 10th fibonacci sequence
        let raw_pub_input = u64::from_le_bytes(fibo_input);
        let res_fr = sp1_generate_scalar_from_raw_public_input(raw_pub_input);
        let wit_fr: Vec<Fr> = load_witness_from_file(&format!("srs_secu/{R1CS_WITNESS_FILE}"));

        assert_eq!(res_fr, wit_fr[2]); // should be equal to the witness that is expected by R1CS
        // In this way we ensure verifier can validate bridge public inputs (e.g. deposit_index) with r1cs constraints
        // [1, 7527402554317099476086310993202889463751940730940407143885949231928, 19542051593079647282099705468191403958371264520862632234952945594121,
    }

    #[test]
    #[ignore]
    fn test_update_to_include_vandermode_matrix_d() {
        let cache_dir = Path::new("srs_secu");
        let (r1cs, wit_fr) = {
            let f = File::open(cache_dir.join(R1CS_CONSTRAINTS_FILE)).unwrap();
            let r1cs = load_sparse_r1cs_from_file(f).unwrap();

            let f = File::open(cache_dir.join(R1CS_WITNESS_FILE)).unwrap();
            let mut src = BufReader::new(f);
            let mut warr = Vector::default();
            warr.read_from(&mut src).unwrap();
            let wit_fr: Vec<Fr> = warr
                .into_par_iter()
                .map(|w| gnark_element_to_fr(&w))
                .collect();
            (r1cs, wit_fr)
        };
        let num_public_inputs = 2;
        let mut inst = R1CSInstance::from_dump(r1cs, num_public_inputs);
        let num_constraints = inst.num_constraints;

        let mut c_vals = vec![Fr::ZERO; num_constraints];
        for (i, c_val) in c_vals.iter_mut().enumerate().take(inst.rows.len()) {
            let row = &inst.rows[i];
            *c_val = R1CSInstance::eval_row(&row.o, &inst.coeffs, &wit_fr);
        }

        // obtain c_vals on domain points
        let tree2n: FFTree<Fr> = read_minimal_fftree_from_file(cache_dir.join(TREE_2N)).unwrap();

        let treen_leaves = &get_both_domains(&tree2n)[0];

        R1CSInstance::update_to_include_vandermode_matrix_d(
            &mut inst,
            treen_leaves,
            num_public_inputs,
        );

        let mut c_vals2 = vec![Fr::ZERO; num_constraints];
        for (i, c_val) in c_vals2.iter_mut().enumerate().take(inst.rows.len()) {
            let row = &inst.rows[i];
            *c_val = R1CSInstance::eval_row(&row.o, &inst.coeffs, &wit_fr);
        }

        // c_vals2 = c_vals - i_vals
        let i_vals2 = {
            // draw a line between two public inputs and domain_points
            let mut i_vals = vec![Fr::ZERO; num_constraints];
            for i in 0..treen_leaves.len() {
                i_vals[i] = wit_fr[2] * treen_leaves[i] + wit_fr[1];
            }
            i_vals
        };

        for i in 0..c_vals.len() {
            assert_eq!(c_vals[i] - i_vals2[i], c_vals2[i])
        }
    }
}
